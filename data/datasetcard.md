---
TODO: Add YAML tags here. Copy-paste the tags obtained with the online tagging app: https://huggingface.co/spaces/huggingface/datasets-tagging
---

# Dataset Card for EAT-IT

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:**
- **Repository:**
- **Paper:**
- **Leaderboard:**
- **Point of Contact:**

### Dataset Summary

The dataset EAT-IT (Energy Accuracy Tradeoff - Interactive Training) is a collection of metrics measured during the training of a VGG16 pretrained model on 12 different image classification datasets. The metrics are focused on measuring the performance and the energy consumption applying different training designs.

### Supported Tasks and Leaderboards

The dataset cam be used to train a model for series forecasting, using training epochs as the temporal unit.

### Languages

[pass]

## Dataset Structure

### Data Instances

The dataset contains 4 files of tabular data: ```history.csv``` generated by Tensorflow, ```emissions.csv``` generated by CodeCarbon, ```monitor.csv``` generated by Nvidia, and ```datasets.csv``` manually generated.

Every instance of ```history.csv``` contains information about the training setup which includes the dataset, the training mode, the epoch of intervention of the mode and the current epoch; and the performance metrics at the end of the epoch, which are loss, accuracy, precision and recall for training and for test splits. An example might look like this:

```
{'dataset':'cars196', 'mode': 'freeze', 'intervention':10, 'epoch':23,
'loss':3.0994184017181396, 'accuracy':0.26547151803970337, 'precision':0.7512195110321045, 'recall':0.09454813599586487,
'val_loss':5.079755783081055, 'val_accuracy':0.08431787043809891, 'val_precision':0.27421554923057556, 'val_accuracy':0.024996891617774963}
```

Every instance of ```emissions.csv``` starts with a timestamp, a name, an id of the run, and all the attributes collected by the CodeCarbon library. Those are duration of the experiment, the emissions and emission rate, the power and energy consumed by GPU, CPU & RAM, as well as the total energy consumed, and additional information about the setup of the execution which is the same for all instances. The name connects to `history.csv` by having the former the first attributes of the latter concatenated by an underscore. An example might look like this:

```
{'timestamp':'2023-01-09T09:11:37', 'project_name':'cars196_freeze_10_32', 'run_id':'8c6554d9-146a-484e-92ae-545f90fac6c0',
'duration':19.55484914779663, 'emissions':0.00030326451976631953, 'emissions_rate':0.015508404972814135, 'cpu_power':60.0,'gpu_power':234.31000000000003,'ram_power':0.375,'cpu_energy':0.00032589763005574544, 'gpu_energy':0.0012684436471059588, 'ram_energy':1.7877742399772008e-06, 'energy_consumed':0.0015961290514016817,
'country_name':'Spain', 'country_iso_code':'ESP', 'region':'catalonia', 'cloud_provider':'', 'cloud_region':'', 'os':'Linux-4.15.0-72-generic-x86_64-with-glibc2.27', 'python_version':'3.9.13', 'cpu_count':1, 'cpu_model':'AMD EPYC 7282 16-Core Processor', 'gpu_count':5, 'gpu_model':'5 x NVIDIA GeForce RTX 3090', 'longitude':'', 'latitude':'', 'ram_total_size':1, 'tracking_mode':'machine','on_cloud':'N'}
```
The instances in ```monitor.csv``` contain a timestamp, the usage of GPU and memory, the power draw and the temperature of the GPU. The metrics are taken every second, so that is the expected difference between adjacent instances, excepting for resets of execution. Multiple instances relate to each instance of `emissions.csv` if the timestamps of the former fall between the end of the process of that instance of the latter (its timestamp) and the start (its timestamp subtractict its duration). Some instances might not relate to any oher in `monitor.csv`. An instance might look like this:

```
{'timestamp': '2023/01/09 09:11:56.457', 'utilization.gpu':51%, 'utilization.memory':24%, 'memory.total':24268, 'memory.used':23559, 'power.draw':198.03, 'temperature.gpu':52}
```

### Data Fields

`**history.csv**`
- `dataset`: a string containing the dataset as named by TensorFlow
- `mode`: a string containing the training mode of the current epoch, between `freeze`, `quant` or `base`
- `intervention`: an integer marking the epoch at which the current mode was applied, 0 if no mode has been applied yet (`base` mode)
- `epoch`: an integer marking the current epoch of the training
- `loss`, `accuracy`, `precision`, `recall`: floats representing the metrics evaluated on the train split at the end of each epoch
- `val_loss`, `val_accuracy`, `val_precision`, `val_recall`: floats representing the metrics evaluated on the validation split at the end of each epoch

`**emissions.csv**`:
- `timestamp`: a string containing the timestamp at which the instance was created in `%Y-%m-%dT%H:%M:%S` format
- `project_name`: a string containing the four attributes of the training setup (that is, dataset, training mode, epoch of intervention and epoch of training) concatenated by an underscore
- `run_id`: an hexadecimal string with the id of the instance
- `duration`: duration of the compute, in seconds
- `emissions`: emissions as CO₂-equivalents [CO₂eq], in kg
- `emissions_rate`: emissions divided per duration, in Kg/s
- `cpu_power`, `gpu_power`, `ram_power`: power consumed by the devices, in watts (W)
- `cpu_energy`, `gpu_energy`, `ram_energy`: energy consumed by the devices, in kilowatts-hour (kW h)
- `energy_consumed`: sum of `cpu_energy`, `gpu_energy` and `ram_energy` (kW-h)
- `country_name`: name of the country where the infrastructure is hosted, always `Spain` in this dataset
- `country_iso_code`: 3-letter alphabet ISO Code of the respective country, always `ESP` in this dataset
- `region`: province/state/city where the compute infrastructure is hosted, always `Catalonia` in this dataset
- `cloud_provider`, `cloud_region`: characteristics of the cloud computing service, always blank in this dataset as no cloud computing service has been used
- `os`: a string with the OS of the device of execution, always `Linux-4.15.0-72-generic-x86_64-with-glibc2.27` in this dataset
- `python_version`: a string with the version of python that is being used, always `3.9.13` in this datset
- `cpu_count`, `cpu_model`, `gpu_count`, `gpu_model`: number and model of the devices used, always 1, `AMD EPYC 7282 16-Core Processor`, 5, `5 x NVIDIA GeForce RTX 3090` in this dataset
- `longitude`, `latitude`: location of the device during the execution, always blank in this dataset as it was performed offline
- `ram_total_size`: total RAM available, in Gb, always 1 in this dataset
- `tracking_mode`: always `machine` in this dataset
- `on_cloud`: a bool in `Y/N` marking if the infrastructure is in cloud

`**monitor.csv**`
- `timestamp`: a string containing the timestamp at the time nvidia-smi was invoked in format `%Y/%m/%d %H:%M:%S.%f`
- `utilization.gpu`, `utilization.memory: an integer representing the percentage of use of the devices
- `memory.total`, `memory.used`: an integer representing the total and used memory, in MiB
- `power.draw`: a 2-decimal number representing  the last measured power draw for the entire board, in W
- `temperature.gpu`: an integer representing the temperature of the device in ºC

### Data Splits

There is no specified split, although the dataset comes from 12 different previous datasets, so a k-fold cross-validation can be performed using 12 splits.

## Dataset Creation

### Curation Rationale

This dataset was developed to study the tradeoff between model performance and energy consumption with different methods of training and how does the moment at which the method is applied affects such tradeoff. 

### Source Data

The data was generated by training a VGG16 model pretrained on Imagenet over 12 different datasets, the same 12 used in [here](https://arxiv.org/abs/1805.08974).

#### Initial Data Collection and Normalization

#### Who are the source language producers?

[More Information Needed]

### Annotations

The dataset has no additional annotations

#### Annotation process

[N/A]

#### Who are the annotators?

[N/A]

### Personal and Sensitive Information

The dataset collects information about the device where the process has been executed, such as OS, but no identifiers. The country and region of execution is provided, but as the execution is performed online, no latitude or longitude is collected. I case of being collected, CodeCarbon itself protects this data with a diminished precision of 11.1 km / 123 km².

## Considerations for Using the Data

### Social Impact of Dataset

The purpose of this dataset is to help develop models that suggest the best training design in order to optimize the tradeoff between accuracy and energy consumption. All the methods proposed reduce the expected energy requirements, so the suggestions will never increase the baseline consumption.

### Discussion of Biases

This dataset is all computer-generated, and it doesn't contain the original data nor the model predictions on that data, so the direct biases of the original datasets or the model are eliminated. Nonetheless, they can slightly affect the results of the generation process.

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

The dataset was created by Álvaro Domingo Reguero, under the supervision of Silverio Martínez-Rodríguez and Roberto Verdechia, in the framework of the former's Bachelor’s Degree Thesis conducted at Universitat Politècnica de Catalunya.

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [Álvaro Domingo Reguero](https://github.com/alvarodr21) for adding this dataset.
